package com.example.ai_guardian_companion.guide.viewmodel

import android.app.Application
import android.graphics.Bitmap
import android.hardware.Sensor
import android.hardware.SensorEvent
import android.hardware.SensorEventListener
import android.hardware.SensorManager
import android.util.Log
import androidx.lifecycle.AndroidViewModel
import androidx.lifecycle.viewModelScope
import com.example.ai_guardian_companion.config.AppConfig
import com.example.ai_guardian_companion.data.local.GuardianDatabase
import com.example.ai_guardian_companion.data.model.*
import com.example.ai_guardian_companion.data.repository.GuideRepository
import com.example.ai_guardian_companion.guide.api.GuideOpenAIClient
import com.example.ai_guardian_companion.guide.audio.AudioPlayer
import com.example.ai_guardian_companion.guide.audio.AudioRecorder
import kotlinx.coroutines.Job
import kotlinx.coroutines.delay
import kotlinx.coroutines.flow.MutableStateFlow
import kotlinx.coroutines.flow.StateFlow
import kotlinx.coroutines.flow.asStateFlow
import kotlinx.coroutines.launch
import kotlin.math.sqrt

/**
 * å¯¼ç›² ViewModelï¼ˆå¯¼èˆªæ¨¡å¼ä¸“ç”¨ï¼‰
 */
class GuideViewModel(application: Application) : AndroidViewModel(application), SensorEventListener {

    private val database = GuardianDatabase.getDatabase(application)
    private val repository = GuideRepository(
        database.guideMessageDao(),
        database.guideSessionDao()
    )

    private val appConfig = AppConfig(application)
    private val openAIClient = GuideOpenAIClient(application)
    val audioPlayer = AudioPlayer(application)
    val audioRecorder = AudioRecorder(application)

    private val sensorManager = application.getSystemService(SensorManager::class.java)
    private val accelerometer = sensorManager?.getDefaultSensor(Sensor.TYPE_ACCELEROMETER)

    // çŠ¶æ€æµ
    private val _appState = MutableStateFlow(AppState.IDLE)
    val appState: StateFlow<AppState> = _appState.asStateFlow()

    private val _messages = MutableStateFlow<List<GuideMessage>>(emptyList())
    val messages: StateFlow<List<GuideMessage>> = _messages.asStateFlow()

    private val _isApiConfigured = MutableStateFlow(openAIClient.isConfigured())
    val isApiConfigured: StateFlow<Boolean> = _isApiConfigured.asStateFlow()

    private val _errorMessage = MutableStateFlow<String?>(null)
    val errorMessage: StateFlow<String?> = _errorMessage.asStateFlow()

    private val _currentSamplingSpeed = MutableStateFlow(SamplingSpeed.NORMAL)
    val currentSamplingSpeed: StateFlow<SamplingSpeed> = _currentSamplingSpeed.asStateFlow()

    private val _lastAdvice = MutableStateFlow<String?>(null)
    val lastAdvice: StateFlow<String?> = _lastAdvice.asStateFlow()

    private val _currentHazardLevel = MutableStateFlow<HazardLevel>(HazardLevel.LOW)
    val currentHazardLevel: StateFlow<HazardLevel> = _currentHazardLevel.asStateFlow()

    private val _framesProcessed = MutableStateFlow(0)
    val framesProcessed: StateFlow<Int> = _framesProcessed.asStateFlow()

    // ä¼šè¯ç®¡ç†
    private var currentSessionId: String? = null
    private var frameCounter = 0

    // å¯¼èˆªæ¨¡å¼ç›¸å…³
    private var navigationJob: Job? = null
    private var lastSpeakTime = 0L
    private val speakIntervalMs: Long
        get() = 3000L  // 3ç§’æ’­æŠ¥é—´éš”

    // IMU æ•°æ®
    private var lastAcceleration = FloatArray(3)
    private var movementMagnitude = 0f

    // å¸§å¤„ç†æ§åˆ¶
    private var isProcessingFrame = false
    private var pendingFrame: Bitmap? = null

    companion object {
        private const val TAG = "GuideViewModel"
    }

    init {
        // åŠ è½½æœ€è¿‘çš„æ¶ˆæ¯
        viewModelScope.launch {
            repository.getRecentMessages(50).collect { msgs ->
                _messages.value = msgs
            }
        }
    }

    /**
     * å¼€å§‹å¯¼èˆª
     */
    fun startNavigation() {
        if (_appState.value == AppState.RUNNING) {
            Log.w(TAG, "Navigation already running")
            return
        }

        viewModelScope.launch {
            try {
                _appState.value = AppState.RUNNING

                // åˆ›å»ºæ–°ä¼šè¯
                currentSessionId = repository.createSession(GuideMode.NAVIGATION)
                frameCounter = 0
                lastSpeakTime = 0
                _framesProcessed.value = 0
                isProcessingFrame = false
                pendingFrame = null

                // å¯åŠ¨ IMU ç›‘å¬ï¼ˆè‡ªé€‚åº”é‡‡æ ·ï¼‰
                if (appConfig.adaptiveSampling) {
                    accelerometer?.let {
                        sensorManager?.registerListener(
                            this@GuideViewModel,
                            it,
                            SensorManager.SENSOR_DELAY_NORMAL
                        )
                    }
                }

                Log.d(TAG, "Navigation started, session: $currentSessionId")

                // æ˜¾ç¤ºå¯åŠ¨æç¤º
                addSystemMessage("å¯¼èˆªå·²å¯åŠ¨ï¼Œè¯·å°†æ‰‹æœºæ‘„åƒå¤´å¯¹å‡†å‰æ–¹")
                speakMessage("å¯¼èˆªå·²å¯åŠ¨", HazardLevel.LOW)

            } catch (e: Exception) {
                Log.e(TAG, "Error starting navigation", e)
                _errorMessage.value = "å¯åŠ¨å¯¼èˆªå¤±è´¥: ${e.message}"
                _appState.value = AppState.ERROR
            }
        }
    }

    /**
     * å¤„ç†å¯¼èˆªå¸§ï¼ˆç”± UI å±‚å®šæœŸè°ƒç”¨ï¼‰
     */
    fun processNavigationFrame(bitmap: Bitmap) {
        // å¦‚æœæ­£åœ¨å¤„ç†ï¼Œä¿å­˜ä¸ºå¾…å¤„ç†å¸§ï¼ˆå•é£æ¨¡å¼ï¼‰
        if (isProcessingFrame) {
            pendingFrame = bitmap
            Log.d(TAG, "Frame queued (processing in progress)")
            return
        }

        // å¦‚æœä¸åœ¨è¿è¡ŒçŠ¶æ€ï¼Œå¿½ç•¥
        if (_appState.value != AppState.RUNNING) {
            return
        }

        viewModelScope.launch {
            try {
                isProcessingFrame = true
                _appState.value = AppState.PROCESSING

                frameCounter++
                _framesProcessed.value = frameCounter

                Log.d(TAG, "Processing frame #$frameCounter")

                // è°ƒç”¨ VLM åˆ†æåœºæ™¯
                val vlmResult = openAIClient.analyzeSceneForGuide(
                    bitmap = bitmap,
                    userQuery = null,
                    isNavMode = true,
                    previousAdvice = _lastAdvice.value
                )

                vlmResult.onSuccess { vlm ->
                    _lastAdvice.value = vlm.response
                    _currentHazardLevel.value = vlm.hazardLevel

                    Log.d(TAG, "VLM result: ${vlm.hazardLevel} - ${vlm.response.take(50)}...")

                    // æ£€æŸ¥æ˜¯å¦éœ€è¦æ’­æŠ¥
                    val shouldSpeak = shouldSpeakAdvice(vlm)

                    if (shouldSpeak) {
                        lastSpeakTime = System.currentTimeMillis()
                        speakMessage(vlm.response, vlm.hazardLevel)
                    }

                    // æ·»åŠ æ¶ˆæ¯åˆ°æ—¥å¿—
                    addAssistantMessage(
                        text = vlm.response,
                        hazardLevel = vlm.hazardLevel,
                        vlmLatencyMs = vlm.latencyMs,
                        tokensUsed = vlm.tokensUsed,
                        wasSpoken = shouldSpeak
                    )

                    // æ›´æ–°ä¼šè¯ç»Ÿè®¡
                    updateSessionStats(vlm)
                }

                vlmResult.onFailure { error ->
                    Log.e(TAG, "VLM failed", error)
                    _errorMessage.value = "åœºæ™¯åˆ†æå¤±è´¥: ${error.message}"
                }

            } catch (e: Exception) {
                Log.e(TAG, "Error processing frame", e)
                _errorMessage.value = "å¤„ç†å¸§å¤±è´¥: ${e.message}"
            } finally {
                isProcessingFrame = false
                _appState.value = AppState.RUNNING

                // å¦‚æœæœ‰å¾…å¤„ç†çš„å¸§ï¼Œç«‹å³å¤„ç†
                pendingFrame?.let { frame ->
                    pendingFrame = null
                    processNavigationFrame(frame)
                }
            }
        }
    }

    /**
     * åˆ¤æ–­æ˜¯å¦éœ€è¦æ’­æŠ¥
     */
    private fun shouldSpeakAdvice(vlm: GuideOpenAIClient.VlmResult): Boolean {
        // é«˜é£é™©ï¼šç«‹å³æ’­æŠ¥
        if (vlm.hazardLevel == HazardLevel.HIGH) {
            return true
        }

        // "NO CHANGE"ï¼šä¸æ’­æŠ¥
        if (vlm.response.contains("NO CHANGE", ignoreCase = true)) {
            return false
        }

        // è·ç¦»ä¸Šæ¬¡æ’­æŠ¥æ—¶é—´æ˜¯å¦è¶…è¿‡é—´éš”
        val timeSinceLastSpeak = System.currentTimeMillis() - lastSpeakTime
        if (timeSinceLastSpeak < speakIntervalMs) {
            return false
        }

        // ä¸­ç­‰é£é™©æˆ–ä½é£é™©ï¼šå®šæœŸæ’­æŠ¥
        return true
    }

    /**
     * æ’­æŠ¥æ¶ˆæ¯
     */
    private suspend fun speakMessage(text: String, hazardLevel: HazardLevel) {
        try {
            _appState.value = AppState.SPEAKING

            Log.d(TAG, "Speaking: $text")

            val ttsResult = openAIClient.synthesizeSpeech(text)

            ttsResult.onSuccess { tts ->
                // æ ¹æ®å±é™©ç­‰çº§é€‰æ‹©æ’­æ”¾ä¼˜å…ˆçº§
                val priority = when (hazardLevel) {
                    HazardLevel.HIGH -> AudioPlayer.Priority.URGENT
                    HazardLevel.MEDIUM -> AudioPlayer.Priority.NORMAL
                    HazardLevel.LOW -> AudioPlayer.Priority.LOW
                }

                audioPlayer.play(tts.audioData, priority)
            }

            ttsResult.onFailure { error ->
                Log.e(TAG, "TTS failed", error)
            }

            // ç­‰å¾…ä¸€å°æ®µæ—¶é—´è®©æ’­æ”¾å¼€å§‹
            delay(200)
            _appState.value = AppState.RUNNING

        } catch (e: Exception) {
            Log.e(TAG, "Error in speakMessage", e)
            _appState.value = AppState.RUNNING
        }
    }

    /**
     * åœæ­¢å¯¼èˆª
     */
    fun stopNavigation() {
        viewModelScope.launch {
            try {
                navigationJob?.cancel()
                navigationJob = null

                // åœæ­¢ IMU ç›‘å¬
                sensorManager?.unregisterListener(this@GuideViewModel)

                // åœæ­¢æ’­æ”¾
                audioPlayer.clearQueue()
                audioPlayer.stopCurrentPlayback()

                // ç»“æŸä¼šè¯
                currentSessionId?.let {
                    repository.endSession(it)
                }

                addSystemMessage("å¯¼èˆªå·²åœæ­¢")
                _appState.value = AppState.IDLE
                _framesProcessed.value = 0

                Log.d(TAG, "Navigation stopped, processed $frameCounter frames")
            } catch (e: Exception) {
                Log.e(TAG, "Error stopping navigation", e)
            }
        }
    }

    /**
     * æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
     */
    private suspend fun addSystemMessage(text: String) {
        val message = GuideMessage(
            mode = GuideMode.NAVIGATION,
            role = "system",
            text = text,
            sessionId = currentSessionId ?: ""
        )
        repository.addMessage(message)
    }

    /**
     * æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯
     */
    private suspend fun addAssistantMessage(
        text: String,
        hazardLevel: HazardLevel,
        vlmLatencyMs: Long,
        tokensUsed: Int,
        wasSpoken: Boolean
    ) {
        val message = GuideMessage(
            mode = GuideMode.NAVIGATION,
            role = "assistant",
            text = text,
            hasImage = true,
            frameId = frameCounter,
            hazardLevel = hazardLevel,
            vlmLatencyMs = vlmLatencyMs,
            tokensUsed = tokensUsed,
            sessionId = currentSessionId ?: ""
        )
        repository.addMessage(message)
    }

    /**
     * æ›´æ–°ä¼šè¯ç»Ÿè®¡
     */
    private suspend fun updateSessionStats(vlm: GuideOpenAIClient.VlmResult) {
        currentSessionId?.let { sessionId ->
            val hazardCounts = when (vlm.hazardLevel) {
                HazardLevel.HIGH -> Triple(1, 0, 0)
                HazardLevel.MEDIUM -> Triple(0, 1, 0)
                HazardLevel.LOW -> Triple(0, 0, 1)
            }

            repository.updateSessionStats(
                sessionId = sessionId,
                totalTokens = vlm.tokensUsed,
                framesProcessed = 1,
                hazardCounts = hazardCounts
            )
        }
    }

    /**
     * IMU ä¼ æ„Ÿå™¨å›è°ƒï¼ˆè‡ªé€‚åº”é‡‡æ ·ï¼‰
     */
    override fun onSensorChanged(event: SensorEvent?) {
        event?.let {
            if (it.sensor.type == Sensor.TYPE_ACCELEROMETER) {
                val x = it.values[0]
                val y = it.values[1]
                val z = it.values[2]

                // è®¡ç®—åŠ é€Ÿåº¦å˜åŒ–
                val deltaX = x - lastAcceleration[0]
                val deltaY = y - lastAcceleration[1]
                val deltaZ = z - lastAcceleration[2]

                movementMagnitude = sqrt(deltaX * deltaX + deltaY * deltaY + deltaZ * deltaZ)

                // æ›´æ–°é‡‡æ ·é€Ÿåº¦
                _currentSamplingSpeed.value = when {
                    movementMagnitude > 5.0f -> SamplingSpeed.FAST    // å¿«é€Ÿç§»åŠ¨
                    movementMagnitude > 2.0f -> SamplingSpeed.NORMAL  // æ­£å¸¸ç§»åŠ¨
                    else -> SamplingSpeed.SLOW                        // é™æ­¢/æ…¢é€Ÿ
                }

                lastAcceleration[0] = x
                lastAcceleration[1] = y
                lastAcceleration[2] = z
            }
        }
    }

    override fun onAccuracyChanged(sensor: Sensor?, accuracy: Int) {
        // Not used
    }

    /**
     * è·å–å½“å‰é‡‡æ ·é—´éš”ï¼ˆæ¯«ç§’ï¼‰
     */
    fun getSamplingIntervalMs(): Long {
        return if (appConfig.adaptiveSampling) {
            // è‡ªé€‚åº”é‡‡æ ·
            when (_currentSamplingSpeed.value) {
                SamplingSpeed.SLOW -> 2000L      // 0.5 fps
                SamplingSpeed.NORMAL -> 1000L    // 1 fps
                SamplingSpeed.FAST -> 500L       // 2 fps
            }
        } else {
            // å›ºå®šé‡‡æ ·ç‡
            (1000f / appConfig.navigationSamplingRate).toLong()
        }
    }

    /**
     * æ¸…é™¤é”™è¯¯æ¶ˆæ¯
     */
    fun clearError() {
        _errorMessage.value = null
    }

    /**
     * å¼€å§‹è¯­éŸ³æé—®ï¼ˆå¯¼èˆªæ—¶ï¼‰
     */
    fun startVoiceQuestion(currentFrame: Bitmap?) {
        Log.d(TAG, "ğŸ“¢ startVoiceQuestion called, appState=${_appState.value}")

        if (_appState.value != AppState.RUNNING) {
            Log.w(TAG, "âŒ Can only ask questions during navigation (current state: ${_appState.value})")
            return
        }

        viewModelScope.launch {
            try {
                Log.d(TAG, "ğŸ¤ Changing state to LISTENING...")
                _appState.value = AppState.LISTENING

                Log.d(TAG, "ğŸ¤ Calling audioRecorder.startRecording()...")
                val result = audioRecorder.startRecording()

                result.onSuccess {
                    Log.d(TAG, "âœ… Recording started successfully")
                }

                result.onFailure { error ->
                    Log.e(TAG, "âŒ Failed to start recording: ${error.message}", error)
                    _errorMessage.value = "å½•éŸ³å¤±è´¥: ${error.message}"
                    _appState.value = AppState.RUNNING
                }
            } catch (e: Exception) {
                Log.e(TAG, "âŒ Exception in startVoiceQuestion: ${e.message}", e)
                _errorMessage.value = "å¯åŠ¨è¯­éŸ³å¤±è´¥: ${e.message}"
                _appState.value = AppState.RUNNING
            }
        }
    }

    /**
     * åœæ­¢è¯­éŸ³æé—®å¹¶å¤„ç†
     */
    fun stopVoiceQuestion(currentFrame: Bitmap?) {
        viewModelScope.launch {
            try {
                _appState.value = AppState.TRANSCRIBING

                // åœæ­¢å½•éŸ³
                val recordingResult = audioRecorder.stopRecording()
                recordingResult.onFailure { error ->
                    Log.e(TAG, "Failed to stop recording", error)
                    _errorMessage.value = "åœæ­¢å½•éŸ³å¤±è´¥: ${error.message}"
                    _appState.value = AppState.RUNNING
                    return@launch
                }

                val audioFile = recordingResult.getOrNull()!!
                Log.d(TAG, "Audio file: ${audioFile.absolutePath}, size: ${audioFile.length()}")

                // ASR: è½¬å†™è¯­éŸ³
                val asrResult = openAIClient.transcribeAudio(audioFile)

                asrResult.onFailure { error ->
                    Log.e(TAG, "ASR failed", error)
                    _errorMessage.value = "è¯­éŸ³è¯†åˆ«å¤±è´¥: ${error.message}"
                    _appState.value = AppState.RUNNING
                    audioFile.delete()
                    return@launch
                }

                val asr = asrResult.getOrNull()!!
                val userQuestion = asr.transcript

                Log.d(TAG, "User question: '$userQuestion' (${asr.latencyMs}ms)")

                // åˆ é™¤éŸ³é¢‘æ–‡ä»¶
                audioFile.delete()

                // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
                addUserMessage(userQuestion, currentFrame != null, asr.latencyMs)

                // å¦‚æœæœ‰å½“å‰å¸§ï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™å°è¯•æŠ“æ‹
                val frameToUse = currentFrame
                if (frameToUse != null) {
                    processUserQuestion(userQuestion, frameToUse)
                } else {
                    Log.w(TAG, "No frame available for user question")
                    _appState.value = AppState.SPEAKING
                    val response = "æŠ±æ­‰ï¼Œå½“å‰æ²¡æœ‰ç”»é¢å¯ä»¥åˆ†æã€‚æ‚¨è¯´ï¼š$userQuestion"
                    speakMessage(response, HazardLevel.LOW)
                    addAssistantMessage(response, HazardLevel.LOW, 0, 0, true)
                }
            } catch (e: Exception) {
                Log.e(TAG, "Error in stopVoiceQuestion", e)
                _errorMessage.value = "å¤„ç†è¯­éŸ³å¤±è´¥: ${e.message}"
                _appState.value = AppState.RUNNING
            }
        }
    }

    /**
     * å¤„ç†ç”¨æˆ·æé—®
     */
    private suspend fun processUserQuestion(userQuestion: String, frame: Bitmap) {
        try {
            _appState.value = AppState.PROCESSING

            frameCounter++
            _framesProcessed.value = frameCounter

            Log.d(TAG, "Processing user question with frame #$frameCounter")

            // VLM: åˆ†æåœºæ™¯å¹¶å›ç­”é—®é¢˜
            val vlmResult = openAIClient.analyzeSceneForGuide(
                bitmap = frame,
                userQuery = userQuestion,
                isNavMode = false,  // ç”¨æˆ·æé—®æ¨¡å¼
                previousAdvice = null
            )

            vlmResult.onSuccess { vlm ->
                _lastAdvice.value = vlm.response
                _currentHazardLevel.value = vlm.hazardLevel

                Log.d(TAG, "VLM answer: ${vlm.response.take(50)}...")

                // æ’­æŠ¥å›ç­”
                speakMessage(vlm.response, vlm.hazardLevel)

                // æ·»åŠ æ¶ˆæ¯åˆ°æ—¥å¿—
                addAssistantMessage(
                    text = vlm.response,
                    hazardLevel = vlm.hazardLevel,
                    vlmLatencyMs = vlm.latencyMs,
                    tokensUsed = vlm.tokensUsed,
                    wasSpoken = true
                )

                // æ›´æ–°ä¼šè¯ç»Ÿè®¡
                updateSessionStats(vlm)
            }

            vlmResult.onFailure { error ->
                Log.e(TAG, "VLM failed for user question", error)
                _errorMessage.value = "åœºæ™¯åˆ†æå¤±è´¥: ${error.message}"
                _appState.value = AppState.RUNNING
            }

        } catch (e: Exception) {
            Log.e(TAG, "Error processing user question", e)
            _errorMessage.value = "å¤„ç†é—®é¢˜å¤±è´¥: ${e.message}"
            _appState.value = AppState.RUNNING
        }
    }

    /**
     * æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
     */
    private suspend fun addUserMessage(text: String, hasImage: Boolean, asrLatencyMs: Long) {
        val message = GuideMessage(
            mode = GuideMode.NAVIGATION,
            role = "user",
            text = text,
            hasImage = hasImage,
            frameId = frameCounter,
            asrLatencyMs = asrLatencyMs,
            sessionId = currentSessionId ?: ""
        )
        repository.addMessage(message)
    }

    /**
     * æ¸…ç†èµ„æº
     */
    override fun onCleared() {
        super.onCleared()
        audioPlayer.release()
        audioRecorder.release()
        sensorManager?.unregisterListener(this)
        navigationJob?.cancel()
    }
}
