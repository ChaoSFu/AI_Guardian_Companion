package com.example.ai_guardian_companion.conversation

import android.content.Context
import android.util.Log
import androidx.lifecycle.LifecycleOwner
import com.example.ai_guardian_companion.audio.*
import com.example.ai_guardian_companion.camera.CameraManager
import com.example.ai_guardian_companion.camera.ImageProcessor
import com.example.ai_guardian_companion.openai.*
import com.example.ai_guardian_companion.storage.ConversationDatabase
import com.example.ai_guardian_companion.storage.FileManager
import com.example.ai_guardian_companion.storage.entity.*
import com.example.ai_guardian_companion.ui.model.ConversationMessage
import com.example.ai_guardian_companion.ui.model.SessionStats
import kotlinx.coroutines.*
import kotlinx.coroutines.flow.*

/**
 * å¯¹è¯ç®¡ç†å™¨
 *
 * åŠŸèƒ½ï¼š
 * - åè°ƒæ‰€æœ‰å­ç³»ç»Ÿï¼ˆéŸ³é¢‘ã€è§†é¢‘ã€WebSocketã€å­˜å‚¨ï¼‰
 * - ç®¡ç†å¯¹è¯çŠ¶æ€æœº
 * - å¤„ç† VAD äº‹ä»¶å’ŒçŠ¶æ€è½¬æ¢
 * - ç®¡ç† turn ç”Ÿå‘½å‘¨æœŸ
 * - å­˜å‚¨å¯¹è¯æ•°æ®
 */
class ConversationManager(
    private val context: Context,
    private val lifecycleOwner: LifecycleOwner,
    private val apiKey: String
) {
    companion object {
        private const val TAG = "ConversationManager"
        private const val MAX_IMAGES_PER_TURN = RealtimeConfig.Image.MAX_IMAGES_PER_TURN  // 3
    }

    private val scope = CoroutineScope(Dispatchers.Main + SupervisorJob())

    // å­ç³»ç»Ÿ
    private val audioInputManager = AudioInputManager(context)
    private val audioOutputManager = AudioOutputManager()
    private val vadDetector = VadDetector()
    private val cameraManager = CameraManager(context, lifecycleOwner)
    private lateinit var realtimeWebSocket: RealtimeWebSocket

    // å­˜å‚¨
    private val database = ConversationDatabase.getDatabase(context)
    private val fileManager = FileManager(context)

    // çŠ¶æ€æœº
    private val stateMachine = ConversationStateMachine()

    // çŠ¶æ€æµ
    private val _conversationState = MutableStateFlow(ConversationState.IDLE)
    val conversationState: StateFlow<ConversationState> = _conversationState

    // å¯¹è¯æ¶ˆæ¯æµ
    private val _messages = MutableStateFlow<List<ConversationMessage>>(emptyList())
    val messages: StateFlow<List<ConversationMessage>> = _messages.asStateFlow()

    // ä¼šè¯ç»Ÿè®¡æµ
    private val _sessionStats = MutableStateFlow(SessionStats())
    val sessionStats: StateFlow<SessionStats> = _sessionStats.asStateFlow()

    // å½“å‰ä¼šè¯
    private var currentSessionId: String? = null
    private var currentUserTurnId: Long? = null
    private var currentModelTurnId: Long? = null

    // è®¡æ•°å™¨
    private var userAudioIndex = 0
    private var modelAudioIndex = 0
    private var imageIndex = 0

    // ç¼“å†²
    private val userAudioChunks = mutableListOf<ByteArray>()
    private val modelAudioChunks = mutableListOf<ByteArray>()
    private val capturedImages = mutableListOf<ImageProcessor.ProcessedImage>()

    /**
     * åˆå§‹åŒ–
     */
    fun initialize() {
        // åˆå§‹åŒ– WebSocket
        realtimeWebSocket = RealtimeWebSocket(apiKey, createWebSocketCallback())

        // åˆå§‹åŒ–éŸ³é¢‘è¾“å‡º
        audioOutputManager.initialize()

        // ç›‘å¬éŸ³é¢‘è¾“å…¥
        scope.launch {
            audioInputManager.audioFlow.collect { audioChunk ->
                handleAudioInput(audioChunk)
            }
        }

        // ç›‘å¬ VAD äº‹ä»¶
        scope.launch {
            vadDetector.vadEvents.collect { vadEvent ->
                handleVadEvent(vadEvent)
            }
        }

        // ç›‘å¬ç›¸æœºå¸§
        scope.launch {
            cameraManager.frameFlow.collect { frame ->
                handleCameraFrame(frame)
            }
        }

        // ç›‘å¬çŠ¶æ€æœºå˜åŒ–
        scope.launch {
            stateMachine.stateFlow.collect { state ->
                _conversationState.value = state
                Log.d(TAG, "State changed: $state")
            }
        }

        Log.d(TAG, "âœ… ConversationManager initialized")
    }

    /**
     * å¯åŠ¨ç›¸æœºé¢„è§ˆ
     */
    suspend fun startCamera(previewView: androidx.camera.view.PreviewView) {
        cameraManager.startCamera(previewView)
    }

    /**
     * å¼€å§‹ä¼šè¯
     */
    suspend fun startSession(): Result<String> {
        return try {
            // ç”Ÿæˆä¼šè¯ ID
            val sessionId = "session_${System.currentTimeMillis()}"
            currentSessionId = sessionId

            // åˆ›å»ºä¼šè¯ç›®å½•
            fileManager.createSessionDirectory(sessionId)

            // åˆ›å»ºä¼šè¯è®°å½•
            val session = SessionEntity(
                sessionId = sessionId,
                startTime = System.currentTimeMillis(),
                deviceInfo = android.os.Build.MODEL,
                modelName = RealtimeConfig.MODEL_NAME
            )
            database.sessionDao().insertSession(session)

            // è¿æ¥ WebSocket
            realtimeWebSocket.connect()

            // ç­‰å¾…è¿æ¥æˆåŠŸ
            realtimeWebSocket.connectionState
                .first { it == RealtimeWebSocket.ConnectionState.Connected }

            // æ›´æ–°ä¼šè¯é…ç½®
            val sessionUpdate = ClientMessage.SessionUpdate(
                session = ClientMessage.SessionUpdate.Session(
                    instructions = RealtimeConfig.SYSTEM_PROMPT,
                    inputAudioTranscription = ClientMessage.SessionUpdate.InputAudioTranscription(),
                    turnDetection = null  // ä½¿ç”¨å®¢æˆ·ç«¯ VAD
                )
            )
            realtimeWebSocket.send(sessionUpdate)

            // å¼€å§‹éŸ³é¢‘è¾“å…¥
            audioInputManager.startRecording()

            // å¼€å§‹éŸ³é¢‘è¾“å‡º
            audioOutputManager.startPlayback()

            // å¼€å§‹ç¯å¢ƒå¸§æ•è·
            cameraManager.startAmbientCapture()

            Log.d(TAG, "âœ… Session started: $sessionId")
            Result.success(sessionId)
        } catch (e: Exception) {
            Log.e(TAG, "Failed to start session", e)
            Result.failure(e)
        }
    }

    /**
     * ç»“æŸä¼šè¯
     */
    suspend fun endSession() = withContext(Dispatchers.IO) {
        val sessionId = currentSessionId ?: return@withContext

        try {
            // åœæ­¢æ‰€æœ‰å­ç³»ç»Ÿ
            audioInputManager.stopRecording()
            audioOutputManager.stopPlayback()
            cameraManager.stopAmbientCapture()
            realtimeWebSocket.disconnect()

            // ç»“æŸå½“å‰ turn
            endCurrentUserTurn()
            endCurrentModelTurn()

            // æ›´æ–°ä¼šè¯è®°å½•
            val endTime = System.currentTimeMillis()
            val session = database.sessionDao().getSessionById(sessionId)
            if (session != null) {
                val duration = endTime - session.startTime
                database.sessionDao().endSession(sessionId, endTime, duration)
            }

            Log.d(TAG, "âœ… Session ended: $sessionId")
        } catch (e: Exception) {
            Log.e(TAG, "Failed to end session", e)
        } finally {
            currentSessionId = null
            stateMachine.reset()
        }
    }

    /**
     * å¤„ç†éŸ³é¢‘è¾“å…¥
     */
    private suspend fun handleAudioInput(audioChunk: AudioInputManager.AudioChunk) {
        // VAD å¤„ç†
        vadDetector.processAudioChunk(audioChunk)

        // æ ¹æ®çŠ¶æ€å†³å®šæ˜¯å¦å‘é€éŸ³é¢‘
        when (_conversationState.value) {
            ConversationState.LISTENING -> {
                // ç”¨æˆ·æ­£åœ¨è¯´è¯ï¼Œç¼“å†²éŸ³é¢‘
                userAudioChunks.add(audioChunk.data)

                // å‘é€åˆ° WebSocket
                val base64Audio = AudioProcessor.pcm16ToBase64(audioChunk.data)
                val message = ClientMessage.InputAudioBufferAppend(audio = base64Audio)
                val sent = realtimeWebSocket.send(message)
                if (!sent) {
                    Log.w(TAG, "âš ï¸ Failed to send audio chunk")
                }
            }
            ConversationState.MODEL_SPEAKING -> {
                // æ¨¡å‹æ­£åœ¨è¯´è¯ï¼Œæ£€æµ‹æ‰“æ–­
                // VAD ä¼šè‡ªåŠ¨è§¦å‘ INTERRUPTING çŠ¶æ€
            }
            else -> {
                // IDLE or INTERRUPTING: ä¸å‘é€éŸ³é¢‘
                // Log at VERBOSE level to avoid spam
                Log.v(TAG, "â¸ï¸ Not sending audio in state: ${_conversationState.value}")
            }
        }
    }

    /**
     * å¤„ç† VAD äº‹ä»¶
     */
    private suspend fun handleVadEvent(vadEvent: VadDetector.VadEvent) {
        when (vadEvent) {
            is VadDetector.VadEvent.SpeechStart -> {
                Log.i(TAG, "ğŸ¤ VAD: Speech START detected at ${vadEvent.timestamp}")
                handleSpeechStart(vadEvent.timestamp)
            }
            is VadDetector.VadEvent.SpeechEnd -> {
                Log.i(TAG, "ğŸ¤ VAD: Speech END detected at ${vadEvent.timestamp}")
                handleSpeechEnd(vadEvent.timestamp)
            }
        }
    }

    /**
     * å¤„ç†è¯­éŸ³å¼€å§‹
     */
    private suspend fun handleSpeechStart(timestamp: Long) {
        val currentState = _conversationState.value
        Log.d(TAG, "ğŸ”„ handleSpeechStart in state: $currentState")

        when (currentState) {
            ConversationState.IDLE -> {
                // IDLE â†’ LISTENING
                Log.i(TAG, "ğŸ“¢ Transition: IDLE â†’ LISTENING (user started speaking)")
                stateMachine.handleEvent(ConversationEvent.VadStart)
                startUserTurn(timestamp)

                // æ•è·é”šç‚¹å¸§
                cameraManager.captureAnchorFrame()

                // è®°å½•äº‹ä»¶
                recordEvent("VAD_START", "User started speaking")
            }
            ConversationState.MODEL_SPEAKING -> {
                // MODEL_SPEAKING â†’ INTERRUPTING
                Log.i(TAG, "ğŸ“¢ Transition: MODEL_SPEAKING â†’ INTERRUPTING (user interrupted)")
                stateMachine.handleEvent(ConversationEvent.UserInterrupt)

                // å–æ¶ˆæ¨¡å‹å›åº”
                realtimeWebSocket.send(ClientMessage.ResponseCancel())

                // æ¸…ç©ºéŸ³é¢‘è¾“å‡ºé˜Ÿåˆ—
                audioOutputManager.flush()

                // ç»“æŸæ¨¡å‹ turnï¼ˆæ ‡è®°ä¸ºè¢«æ‰“æ–­ï¼‰
                endCurrentModelTurn(interrupted = true)

                // å¼€å§‹æ–°çš„ç”¨æˆ· turn
                startUserTurn(timestamp)

                // æ•è·é”šç‚¹å¸§
                cameraManager.captureAnchorFrame()

                // è®°å½•äº‹ä»¶
                recordEvent("INTERRUPT", "User interrupted model")
            }
            else -> {
                // å…¶ä»–çŠ¶æ€ä¸å¤„ç†
                Log.w(TAG, "âš ï¸ Ignoring speech start in state: $currentState")
            }
        }
    }

    /**
     * å¤„ç†è¯­éŸ³åœæ­¢
     */
    private suspend fun handleSpeechEnd(timestamp: Long) {
        val currentState = _conversationState.value
        Log.d(TAG, "ğŸ”„ handleSpeechEnd in state: $currentState")

        when (currentState) {
            ConversationState.LISTENING -> {
                // LISTENING â†’ MODEL_SPEAKING (ç­‰å¾…æ¨¡å‹å›åº”)
                Log.i(TAG, "ğŸ“¢ Transition: LISTENING â†’ MODEL_SPEAKING (requesting response)")
                stateMachine.handleEvent(ConversationEvent.VadEnd)

                // æäº¤éŸ³é¢‘ç¼“å†²
                Log.d(TAG, "ğŸ“¤ Committing audio buffer")
                realtimeWebSocket.send(ClientMessage.InputAudioBufferCommit())

                // å‘é€å›¾åƒï¼ˆå¦‚æœæœ‰ï¼‰
                sendCapturedImages()

                // è¯·æ±‚æ¨¡å‹å›åº”
                Log.d(TAG, "ğŸ“¤ Requesting model response")
                realtimeWebSocket.send(ClientMessage.ResponseCreate())

                // ç»“æŸç”¨æˆ· turn
                endCurrentUserTurn()

                // è®°å½•äº‹ä»¶
                recordEvent("VAD_END", "User stopped speaking")
            }
            else -> {
                // å…¶ä»–çŠ¶æ€ä¸å¤„ç†
                Log.w(TAG, "âš ï¸ Ignoring speech end in state: $currentState")
            }
        }
    }

    /**
     * å¤„ç†ç›¸æœºå¸§
     */
    private suspend fun handleCameraFrame(frame: CameraManager.CapturedFrame) {
        // åªåœ¨ç”¨æˆ· turn æœŸé—´æ”¶é›†å›¾åƒ
        if (_conversationState.value != ConversationState.LISTENING) {
            return
        }

        // é™åˆ¶æ¯ä¸ª turn æœ€å¤š 3 å¼ å›¾åƒ
        if (capturedImages.size >= MAX_IMAGES_PER_TURN) {
            return
        }

        // å¤„ç†å›¾åƒ
        val result = ImageProcessor.processImage(frame.bitmap)
        if (result.isSuccess) {
            val processedImage = result.getOrNull()!!
            capturedImages.add(processedImage)

            // ä¿å­˜å›¾åƒåˆ°æ–‡ä»¶
            val role = when (frame.type) {
                CameraManager.FrameType.AMBIENT -> "ambient"
                CameraManager.FrameType.ANCHOR -> "anchor"
            }
            val sessionId = currentSessionId ?: return
            val imagePath = fileManager.generateImagePath(sessionId, role, imageIndex++)
            val jpegBytes = ImageProcessor.decodeDataUrl(processedImage.dataUrl) ?: return
            fileManager.saveImageFile(sessionId, imagePath, jpegBytes)

            // è®°å½•å›¾åƒåˆ°æ•°æ®åº“
            val turnId = currentUserTurnId ?: return
            val imageEntity = ImageEntity(
                turnId = turnId,
                sessionId = sessionId,
                timestamp = frame.timestamp,
                role = role,
                imagePath = imagePath,
                width = processedImage.width,
                height = processedImage.height,
                quality = processedImage.quality
            )
            database.imageDao().insertImage(imageEntity)

            Log.d(TAG, "Image saved: $imagePath")
        }
    }

    /**
     * å¼€å§‹ç”¨æˆ· turn
     */
    private suspend fun startUserTurn(timestamp: Long) {
        val sessionId = currentSessionId ?: return

        userAudioChunks.clear()
        capturedImages.clear()

        val turn = TurnEntity(
            sessionId = sessionId,
            speaker = "user",
            startTime = timestamp
        )
        currentUserTurnId = database.turnDao().insertTurn(turn)

        Log.d(TAG, "User turn started: $currentUserTurnId")
    }

    /**
     * ç»“æŸç”¨æˆ· turn
     */
    private suspend fun endCurrentUserTurn() = withContext(Dispatchers.IO) {
        val turnId = currentUserTurnId ?: return@withContext
        val sessionId = currentSessionId ?: return@withContext

        // åˆå¹¶éŸ³é¢‘æ•°æ®
        val audioData = AudioProcessor.mergeAudioChunks(userAudioChunks)

        // ä¿å­˜éŸ³é¢‘æ–‡ä»¶
        if (audioData.isNotEmpty()) {
            val audioPath = fileManager.generateAudioPath(sessionId, "user", userAudioIndex++)
            val wavData = AudioProcessor.pcm16ToWav(audioData)
            fileManager.saveAudioFile(sessionId, audioPath, wavData)

            // æ›´æ–° turn è®°å½•
            val endTime = System.currentTimeMillis()
            val turn = database.turnDao().getTurnById(turnId)
            if (turn != null) {
                val duration = endTime - turn.startTime
                database.turnDao().endTurn(turnId, endTime, duration, audioPath)
            }
        }

        currentUserTurnId = null
        Log.d(TAG, "User turn ended: $turnId")
    }

    /**
     * å¼€å§‹æ¨¡å‹ turn
     */
    private suspend fun startModelTurn(timestamp: Long) {
        val sessionId = currentSessionId ?: return

        modelAudioChunks.clear()

        val turn = TurnEntity(
            sessionId = sessionId,
            speaker = "model",
            startTime = timestamp
        )
        currentModelTurnId = database.turnDao().insertTurn(turn)

        Log.d(TAG, "Model turn started: $currentModelTurnId")
    }

    /**
     * ç»“æŸæ¨¡å‹ turn
     */
    private suspend fun endCurrentModelTurn(interrupted: Boolean = false) = withContext(Dispatchers.IO) {
        val turnId = currentModelTurnId ?: return@withContext
        val sessionId = currentSessionId ?: return@withContext

        // åˆå¹¶éŸ³é¢‘æ•°æ®
        val audioData = AudioProcessor.mergeAudioChunks(modelAudioChunks)

        // ä¿å­˜éŸ³é¢‘æ–‡ä»¶
        if (audioData.isNotEmpty()) {
            val audioPath = fileManager.generateAudioPath(sessionId, "model", modelAudioIndex++)
            val wavData = AudioProcessor.pcm16ToWav(audioData)
            fileManager.saveAudioFile(sessionId, audioPath, wavData)

            // æ›´æ–° turn è®°å½•
            val endTime = System.currentTimeMillis()
            val turn = database.turnDao().getTurnById(turnId)
            if (turn != null) {
                val duration = endTime - turn.startTime
                database.turnDao().updateTurn(
                    turn.copy(
                        endTime = endTime,
                        duration = duration,
                        audioPath = audioPath,
                        interrupted = interrupted
                    )
                )
            }
        }

        currentModelTurnId = null
        Log.d(TAG, "Model turn ended: $turnId (interrupted=$interrupted)")
    }

    /**
     * å‘é€æ•è·çš„å›¾åƒ
     */
    private fun sendCapturedImages() {
        if (capturedImages.isEmpty()) {
            return
        }

        val contents = capturedImages.map { image ->
            ClientMessage.ConversationItemCreate.Content(
                type = "image",
                imageUrl = ClientMessage.ConversationItemCreate.ImageUrl(url = image.dataUrl)
            )
        }

        val message = ClientMessage.ConversationItemCreate(
            item = ClientMessage.ConversationItemCreate.Item(
                role = "user",
                content = contents
            )
        )

        realtimeWebSocket.send(message)
        Log.d(TAG, "Sent ${capturedImages.size} images")
    }

    /**
     * è®°å½•äº‹ä»¶
     */
    private suspend fun recordEvent(type: String, detail: String? = null) {
        val sessionId = currentSessionId ?: return

        val event = EventEntity(
            sessionId = sessionId,
            timestamp = System.currentTimeMillis(),
            type = type,
            detail = detail
        )
        database.eventDao().insertEvent(event)
    }

    /**
     * æ·»åŠ æ¶ˆæ¯åˆ°æ˜¾ç¤ºåˆ—è¡¨
     */
    private fun addMessage(speaker: ConversationMessage.Speaker, text: String) {
        val message = ConversationMessage(
            id = "${System.currentTimeMillis()}_${speaker.name}",
            speaker = speaker,
            text = text,
            timestamp = System.currentTimeMillis()
        )
        _messages.value = _messages.value + message
        Log.d(TAG, "Added message: ${speaker.name} - $text")
    }

    /**
     * æ›´æ–°ä¼šè¯ç»Ÿè®¡
     */
    private fun updateSessionStats() {
        val sessionId = currentSessionId ?: return
        _sessionStats.value = _sessionStats.value.copy(
            sessionId = sessionId,
            startTime = _sessionStats.value.startTime ?: System.currentTimeMillis(),
            turnCount = _sessionStats.value.turnCount + 1,
            userTurns = if (currentUserTurnId != null) _sessionStats.value.userTurns + 1 else _sessionStats.value.userTurns,
            modelTurns = if (currentModelTurnId != null) _sessionStats.value.modelTurns + 1 else _sessionStats.value.modelTurns
        )
    }

    /**
     * åˆ›å»º WebSocket å›è°ƒ
     */
    private fun createWebSocketCallback(): RealtimeWebSocket.RealtimeCallback {
        return object : RealtimeWebSocket.RealtimeCallback {
            override fun onConnected() {
                Log.d(TAG, "WebSocket connected")
            }

            override fun onDisconnected(code: Int, reason: String) {
                Log.d(TAG, "WebSocket disconnected: $code - $reason")
            }

            override fun onSessionCreated(message: ServerMessage.SessionCreated) {
                Log.d(TAG, "Session created")
            }

            override fun onSessionUpdated(message: ServerMessage.SessionUpdated) {
                Log.d(TAG, "Session updated")
            }

            override fun onConversationItemCreated(message: ServerMessage.ConversationItemCreated) {
                Log.d(TAG, "Conversation item created")
            }

            override fun onSpeechStarted(message: ServerMessage.InputAudioBufferSpeechStarted) {
                Log.d(TAG, "Server detected speech start")
            }

            override fun onSpeechStopped(message: ServerMessage.InputAudioBufferSpeechStopped) {
                Log.d(TAG, "Server detected speech stop")
            }

            override fun onAudioDelta(message: ServerMessage.ResponseAudioDelta) {
                // è§£ç éŸ³é¢‘æ•°æ®
                val audioData = AudioProcessor.base64ToPcm16(message.delta)
                modelAudioChunks.add(audioData)

                // å†™å…¥éŸ³é¢‘è¾“å‡º
                audioOutputManager.writeAudio(audioData)
            }

            override fun onAudioDone(message: ServerMessage.ResponseAudioDone) {
                Log.d(TAG, "Audio done")
            }

            override fun onTextDelta(message: ServerMessage.ResponseTextDelta) {
                Log.d(TAG, "Text delta: ${message.delta}")
                // æ–‡æœ¬å¢é‡æš‚ä¸æ˜¾ç¤ºï¼Œç­‰å¾…å®Œæ•´æ–‡æœ¬
            }

            override fun onTextDone(message: ServerMessage.ResponseTextDone) {
                Log.d(TAG, "Text done: ${message.text}")
                // æ·»åŠ æ¨¡å‹å›å¤åˆ°æ¶ˆæ¯åˆ—è¡¨
                if (message.text.isNotEmpty()) {
                    addMessage(ConversationMessage.Speaker.MODEL, message.text)
                }
            }

            override fun onResponseDone(message: ServerMessage.ResponseDone) {
                scope.launch {
                    // æ¨¡å‹å›åº”å®Œæˆ
                    stateMachine.handleEvent(ConversationEvent.ModelEnd)
                    endCurrentModelTurn()
                    recordEvent("MODEL_END", "Model finished speaking")
                }
            }

            override fun onServerError(message: ServerMessage.Error) {
                Log.e(TAG, "Server error: ${message.error.message}")
            }

            override fun onError(error: Throwable) {
                Log.e(TAG, "Client error", error)
            }

            override fun onMaxReconnectAttemptsReached() {
                Log.e(TAG, "Max reconnect attempts reached")
            }
        }
    }

    /**
     * é‡Šæ”¾èµ„æº
     */
    fun release() {
        scope.cancel()
        audioInputManager.release()
        audioOutputManager.release()
        vadDetector.release()
        cameraManager.release()
        realtimeWebSocket.release()
        Log.d(TAG, "ConversationManager released")
    }
}
