package com.example.ai_guardian_companion.conversation

import android.content.Context
import android.util.Log
import androidx.lifecycle.LifecycleOwner
import com.example.ai_guardian_companion.audio.*
import com.example.ai_guardian_companion.camera.CameraManager
import com.example.ai_guardian_companion.camera.ImageProcessor
import com.example.ai_guardian_companion.openai.*
import com.example.ai_guardian_companion.storage.ConversationDatabase
import com.example.ai_guardian_companion.storage.FileManager
import com.example.ai_guardian_companion.storage.entity.*
import com.example.ai_guardian_companion.ui.model.ConversationMessage
import com.example.ai_guardian_companion.ui.model.SessionStats
import kotlinx.coroutines.*
import kotlinx.coroutines.flow.*

/**
 * å¯¹è¯ç®¡ç†å™¨
 *
 * åŠŸèƒ½ï¼š
 * - åè°ƒæ‰€æœ‰å­ç³»ç»Ÿï¼ˆéŸ³é¢‘ã€è§†é¢‘ã€WebSocketã€å­˜å‚¨ï¼‰
 * - ç®¡ç†å¯¹è¯çŠ¶æ€æœº
 * - å¤„ç† VAD äº‹ä»¶å’ŒçŠ¶æ€è½¬æ¢
 * - ç®¡ç† turn ç”Ÿå‘½å‘¨æœŸ
 * - å­˜å‚¨å¯¹è¯æ•°æ®
 */
class ConversationManager(
    private val context: Context,
    private val lifecycleOwner: LifecycleOwner,
    private val apiKey: String
) {
    companion object {
        private const val TAG = "ConversationManager"
        private const val MAX_IMAGES_PER_TURN = RealtimeConfig.Image.MAX_IMAGES_PER_TURN  // 3
    }

    private val scope = CoroutineScope(Dispatchers.Main + SupervisorJob())

    // å­ç³»ç»Ÿ
    private val audioInputManager = AudioInputManager(context)
    private val audioOutputManager = AudioOutputManager()
    private val vadDetector = VadDetector()
    private val cameraManager = CameraManager(context, lifecycleOwner)
    private lateinit var realtimeWebSocket: RealtimeWebSocket

    // å­˜å‚¨
    private val database = ConversationDatabase.getDatabase(context)
    private val fileManager = FileManager(context)

    // çŠ¶æ€æœº
    private val stateMachine = ConversationStateMachine()

    // çŠ¶æ€æµ
    private val _conversationState = MutableStateFlow(ConversationState.IDLE)
    val conversationState: StateFlow<ConversationState> = _conversationState

    // å¯¹è¯æ¶ˆæ¯æµ
    private val _messages = MutableStateFlow<List<ConversationMessage>>(emptyList())
    val messages: StateFlow<List<ConversationMessage>> = _messages.asStateFlow()

    // ä¼šè¯ç»Ÿè®¡æµ
    private val _sessionStats = MutableStateFlow(SessionStats())
    val sessionStats: StateFlow<SessionStats> = _sessionStats.asStateFlow()

    // å½“å‰ä¼šè¯
    private var currentSessionId: String? = null
    private var currentUserTurnId: Long? = null
    private var currentModelTurnId: Long? = null

    // è®¡æ•°å™¨
    private var userAudioIndex = 0
    private var modelAudioIndex = 0
    private var imageIndex = 0

    // ç¼“å†²
    private val userAudioChunks = mutableListOf<ByteArray>()
    private val modelAudioChunks = mutableListOf<ByteArray>()
    private val capturedImages = mutableListOf<ImageProcessor.ProcessedImage>()
    private var currentModelText: String? = null  // å½“å‰æ¨¡å‹å›å¤çš„æ–‡æœ¬
    private var currentUserText: String? = null   // å½“å‰ç”¨æˆ·è¯­éŸ³çš„è½¬å½•æ–‡æœ¬
    private var isFirstAudioDelta = true          // æ ‡è®°æ˜¯å¦æ˜¯ç¬¬ä¸€ä¸ªéŸ³é¢‘ delta

    // æ—¥å¿—ä¼˜åŒ–ï¼šè·Ÿè¸ªä¸Šæ¬¡è®°å½•çš„çŠ¶æ€ï¼Œé¿å…é‡å¤æ—¥å¿—
    private var lastLoggedStateForAudio: ConversationState? = null

    /**
     * åˆå§‹åŒ–
     */
    fun initialize() {
        // åˆå§‹åŒ– WebSocket
        realtimeWebSocket = RealtimeWebSocket(apiKey, createWebSocketCallback())

        // åˆå§‹åŒ–éŸ³é¢‘è¾“å‡º
        audioOutputManager.initialize()

        // ç›‘å¬éŸ³é¢‘è¾“å…¥
        scope.launch {
            audioInputManager.audioFlow.collect { audioChunk ->
                handleAudioInput(audioChunk)
            }
        }

        // ç›‘å¬ VAD äº‹ä»¶
        scope.launch {
            vadDetector.vadEvents.collect { vadEvent ->
                handleVadEvent(vadEvent)
            }
        }

        // ç›‘å¬ç›¸æœºå¸§
        scope.launch {
            cameraManager.frameFlow.collect { frame ->
                handleCameraFrame(frame)
            }
        }

        // ç›‘å¬çŠ¶æ€æœºå˜åŒ–
        scope.launch {
            stateMachine.stateFlow.collect { state ->
                _conversationState.value = state
                Log.d(TAG, "State changed: $state")
            }
        }

        Log.d(TAG, "âœ… ConversationManager initialized")
    }

    /**
     * å¯åŠ¨ç›¸æœºé¢„è§ˆ
     */
    suspend fun startCamera(previewView: androidx.camera.view.PreviewView) {
        cameraManager.startCamera(previewView)
    }

    /**
     * å¼€å§‹ä¼šè¯
     */
    suspend fun startSession(): Result<String> {
        return try {
            // ç”Ÿæˆä¼šè¯ ID
            val sessionId = "session_${System.currentTimeMillis()}"
            currentSessionId = sessionId

            // åˆ›å»ºä¼šè¯ç›®å½•
            fileManager.createSessionDirectory(sessionId)

            // åˆ›å»ºä¼šè¯è®°å½•
            val session = SessionEntity(
                sessionId = sessionId,
                startTime = System.currentTimeMillis(),
                deviceInfo = android.os.Build.MODEL,
                modelName = RealtimeConfig.MODEL_NAME
            )
            database.sessionDao().insertSession(session)

            // è¿æ¥ WebSocket
            realtimeWebSocket.connect()

            // ç­‰å¾…è¿æ¥æˆåŠŸ
            realtimeWebSocket.connectionState
                .first { it == RealtimeWebSocket.ConnectionState.Connected }

            // æ›´æ–°ä¼šè¯é…ç½®
            // âœ… æ–¹æ¡ˆAï¼šä¸ä½¿ç”¨ input_audio_bufferï¼Œå› æ­¤ç¦ç”¨ç›¸å…³åŠŸèƒ½
            val sessionUpdate = ClientMessage.SessionUpdate(
                session = ClientMessage.SessionUpdate.Session(
                    instructions = RealtimeConfig.SYSTEM_PROMPT,
                    inputAudioTranscription = null,  // ç¦ç”¨ï¼šåªå¯¹ input_audio_buffer æœ‰æ•ˆ
                    turnDetection = null  // ç¦ç”¨ server_vadï¼Œä½¿ç”¨å®¢æˆ·ç«¯ VAD
                )
            )
            realtimeWebSocket.send(sessionUpdate)

            // å¼€å§‹éŸ³é¢‘è¾“å…¥
            audioInputManager.startRecording()

            // å¼€å§‹éŸ³é¢‘è¾“å‡º
            audioOutputManager.startPlayback()

            // ç­‰å¾…ç›¸æœºå®Œå…¨å°±ç»ªï¼ˆé¿å…ç¬¬ä¸€æ¬¡æ‹ç…§å¤±è´¥ï¼‰
            // ç›¸æœºåˆå§‹åŒ–é€šå¸¸éœ€è¦ 150-200msï¼Œé¢„ç•™ 300ms ç¡®ä¿ç¨³å®š
            delay(300)

            // å¼€å§‹ç¯å¢ƒå¸§æ•è·
            cameraManager.startAmbientCapture()

            Log.d(TAG, "âœ… Session started: $sessionId")
            Result.success(sessionId)
        } catch (e: Exception) {
            Log.e(TAG, "Failed to start session", e)
            Result.failure(e)
        }
    }

    /**
     * ç»“æŸä¼šè¯
     */
    suspend fun endSession() = withContext(Dispatchers.IO) {
        val sessionId = currentSessionId ?: return@withContext

        try {
            // åœæ­¢æ‰€æœ‰å­ç³»ç»Ÿ
            audioInputManager.stopRecording()
            audioOutputManager.stopPlayback()
            cameraManager.stopAmbientCapture()
            realtimeWebSocket.disconnect()

            // ç»“æŸå½“å‰ turn
            endCurrentUserTurn()
            endCurrentModelTurn()

            // æ›´æ–°ä¼šè¯è®°å½•
            val endTime = System.currentTimeMillis()
            val session = database.sessionDao().getSessionById(sessionId)
            if (session != null) {
                val duration = endTime - session.startTime
                database.sessionDao().endSession(sessionId, endTime, duration)
            }

            Log.d(TAG, "âœ… Session ended: $sessionId")
        } catch (e: Exception) {
            Log.e(TAG, "Failed to end session", e)
        } finally {
            currentSessionId = null
            stateMachine.reset()
        }
    }

    /**
     * å¤„ç†éŸ³é¢‘è¾“å…¥
     */
    private suspend fun handleAudioInput(audioChunk: AudioInputManager.AudioChunk) {
        // VAD å¤„ç†
        vadDetector.processAudioChunk(audioChunk)

        // æ ¹æ®çŠ¶æ€å†³å®šæ˜¯å¦ç¼“å†²éŸ³é¢‘
        when (_conversationState.value) {
            ConversationState.LISTENING -> {
                // ç”¨æˆ·æ­£åœ¨è¯´è¯ï¼Œç¼“å†²éŸ³é¢‘ï¼ˆä¸å†æµå¼å‘é€ï¼‰
                userAudioChunks.add(audioChunk.data)

                // é‡ç½®æ—¥å¿—æ ‡å¿—ï¼Œä»¥ä¾¿ä¸‹æ¬¡çŠ¶æ€å˜åŒ–æ—¶èƒ½è®°å½•
                lastLoggedStateForAudio = null

                // âœ… æ–¹æ¡ˆAï¼šåªç¼“å†²ï¼Œä¸å‘é€ã€‚éŸ³é¢‘å°†åœ¨ç”¨æˆ·åœæ­¢è¯´è¯æ—¶å’Œå›¾ç‰‡ä¸€èµ·å‘é€
                Log.v(TAG, "ğŸ™ï¸ Buffering audio chunk (${audioChunk.data.size} bytes), total chunks: ${userAudioChunks.size}")
            }
            ConversationState.MODEL_SPEAKING -> {
                // æ¨¡å‹æ­£åœ¨è¯´è¯ï¼Œæ£€æµ‹æ‰“æ–­
                // VAD ä¼šè‡ªåŠ¨è§¦å‘ INTERRUPTING çŠ¶æ€
            }
            else -> {
                // IDLE or INTERRUPTING: ä¸ç¼“å†²éŸ³é¢‘
                // åªåœ¨çŠ¶æ€å˜åŒ–æ—¶è®°å½•æ—¥å¿—ï¼Œé¿å…æ—¥å¿—æ³›æ»¥
                val currentState = _conversationState.value
                if (lastLoggedStateForAudio != currentState) {
                    Log.v(TAG, "â¸ï¸ Not buffering audio in state: $currentState")
                    lastLoggedStateForAudio = currentState
                }
            }
        }
    }

    /**
     * å¤„ç† VAD äº‹ä»¶
     */
    private suspend fun handleVadEvent(vadEvent: VadDetector.VadEvent) {
        when (vadEvent) {
            is VadDetector.VadEvent.SpeechStart -> {
                Log.i(TAG, "ğŸ¤ VAD: Speech START detected at ${vadEvent.timestamp}")
                handleSpeechStart(vadEvent.timestamp)
            }
            is VadDetector.VadEvent.SpeechEnd -> {
                Log.i(TAG, "ğŸ¤ VAD: Speech END detected at ${vadEvent.timestamp}")
                handleSpeechEnd(vadEvent.timestamp)
            }
        }
    }

    /**
     * å¤„ç†è¯­éŸ³å¼€å§‹
     */
    private suspend fun handleSpeechStart(timestamp: Long) {
        val currentState = _conversationState.value
        Log.d(TAG, "ğŸ”„ handleSpeechStart in state: $currentState")

        when (currentState) {
            ConversationState.IDLE -> {
                // IDLE â†’ LISTENING
                Log.i(TAG, "ğŸ“¢ Transition: IDLE â†’ LISTENING (user started speaking)")
                stateMachine.handleEvent(ConversationEvent.VadStart)
                startUserTurn(timestamp)

                // å¯åŠ¨ç¯å¢ƒå¸§æ•è·
                cameraManager.startAmbientCapture()
                Log.d(TAG, "â–¶ï¸ Started ambient capture (${RealtimeConfig.Image.AMBIENT_FPS} fps)")

                // æ•è·é”šç‚¹å¸§
                cameraManager.captureAnchorFrame()

                // è®°å½•äº‹ä»¶
                recordEvent("VAD_START", "User started speaking")
            }
            ConversationState.MODEL_SPEAKING -> {
                // MODEL_SPEAKING â†’ INTERRUPTING
                Log.i(TAG, "ğŸ“¢ Transition: MODEL_SPEAKING â†’ INTERRUPTING (user interrupted)")
                stateMachine.handleEvent(ConversationEvent.UserInterrupt)

                // å–æ¶ˆæ¨¡å‹å›åº”
                realtimeWebSocket.send(ClientMessage.ResponseCancel())

                // æ¸…ç©ºéŸ³é¢‘è¾“å‡ºé˜Ÿåˆ—ï¼ˆä¸æ¢å¤æ’­æ”¾ï¼‰
                audioOutputManager.flush(resume = false)

                // ç»“æŸæ¨¡å‹ turnï¼ˆæ ‡è®°ä¸ºè¢«æ‰“æ–­ï¼‰
                endCurrentModelTurn(interrupted = true)

                // å¼€å§‹æ–°çš„ç”¨æˆ· turn
                startUserTurn(timestamp)

                // å¯åŠ¨ç¯å¢ƒå¸§æ•è·
                cameraManager.startAmbientCapture()
                Log.d(TAG, "â–¶ï¸ Started ambient capture (${RealtimeConfig.Image.AMBIENT_FPS} fps)")

                // æ•è·é”šç‚¹å¸§
                cameraManager.captureAnchorFrame()

                // è®°å½•äº‹ä»¶
                recordEvent("INTERRUPT", "User interrupted model")
            }
            else -> {
                // å…¶ä»–çŠ¶æ€ä¸å¤„ç†
                Log.w(TAG, "âš ï¸ Ignoring speech start in state: $currentState")
            }
        }
    }

    /**
     * å¤„ç†è¯­éŸ³åœæ­¢
     */
    private suspend fun handleSpeechEnd(timestamp: Long) {
        val currentState = _conversationState.value
        Log.d(TAG, "ğŸ”„ handleSpeechEnd in state: $currentState")

        when (currentState) {
            ConversationState.LISTENING, ConversationState.INTERRUPTING -> {
                // LISTENING â†’ MODEL_SPEAKING (ç­‰å¾…æ¨¡å‹å›åº”)
                // INTERRUPTING â†’ LISTENING â†’ MODEL_SPEAKING (æ‰“æ–­åè¯·æ±‚æ–°å›åº”)
                val transitionDesc = if (currentState == ConversationState.LISTENING) {
                    "LISTENING â†’ MODEL_SPEAKING"
                } else {
                    "INTERRUPTING â†’ LISTENING â†’ MODEL_SPEAKING"
                }
                Log.i(TAG, "ğŸ“¢ Transition: $transitionDesc (requesting response)")
                stateMachine.handleEvent(ConversationEvent.VadEnd)

                // âœ… æ–¹æ¡ˆAï¼šä¸å†ä½¿ç”¨ input_audio_buffer.commit
                // éŸ³é¢‘å°†å’Œå›¾ç‰‡ä¸€èµ·åœ¨ conversation.item.create ä¸­å‘é€
                Log.d(TAG, "ğŸ™ï¸ Audio buffered: ${userAudioChunks.size} chunks")

                // âš ï¸ åœæ­¢ç¯å¢ƒå¸§æ•è·ï¼Œé˜²æ­¢æ–°çš„ç¯å¢ƒå¸§æ··å…¥
                cameraManager.stopAmbientCapture()
                Log.d(TAG, "ğŸ›‘ Stopped ambient capture to prevent frame mixing")

                // æ¸…ç©ºæ—§å›¾ç‰‡ï¼Œåªä½¿ç”¨æœ€æ–°çš„é”šç‚¹å¸§
                capturedImages.clear()
                Log.d(TAG, "ğŸ—‘ï¸ Cleared old images, will capture fresh anchor frame")

                // æ‹æ‘„æœ€æ–°ç”»é¢ï¼ˆç¡®ä¿å‘é€çš„æ˜¯æœ€æ–°å†…å®¹ï¼‰
                Log.d(TAG, "ğŸ“¸ Capturing final anchor frame")
                cameraManager.captureAnchorFrame()

                // ç­‰å¾…é”šç‚¹å¸§å¤„ç†å®Œæˆ
                delay(500)

                Log.d(TAG, "ğŸ“¦ Images ready to send: ${capturedImages.size} images")
                capturedImages.forEachIndexed { index, img ->
                    Log.d(TAG, "  ğŸ“· Image $index: ${img.width}x${img.height}, ${img.sizeBytes} bytes")
                }

                // âœ… å‘é€éŸ³é¢‘ + æ–‡æœ¬æŒ‡ä»¤ + å›¾ç‰‡ï¼ˆåœ¨åŒä¸€ä¸ª conversation.item.create ä¸­ï¼‰
                // âš ï¸ å¿…é¡»ç­‰å¾…å‘é€å®Œæˆåå†è¯·æ±‚å“åº”
                sendAudioAndImages().join()  // âœ… ç­‰å¾…å‘é€å®Œæˆ

                // é‡ç½®æ ‡å¿—ï¼Œå‡†å¤‡æ¥æ”¶æ¨¡å‹å›åº”
                isFirstAudioDelta = true

                // è¯·æ±‚æ¨¡å‹å›åº”ï¼ˆåœ¨å†…å®¹å‘é€å®Œæˆåï¼‰
                Log.d(TAG, "ğŸ“¤ Requesting model response (after content sent)")
                realtimeWebSocket.send(ClientMessage.ResponseCreate())

                // ç»“æŸç”¨æˆ· turn
                endCurrentUserTurn()

                // è®°å½•äº‹ä»¶
                val eventDetail = if (currentState == ConversationState.INTERRUPTING) {
                    "User stopped speaking (after interrupt)"
                } else {
                    "User stopped speaking"
                }
                recordEvent("VAD_END", eventDetail)
            }
            else -> {
                // å…¶ä»–çŠ¶æ€ä¸å¤„ç†
                Log.w(TAG, "âš ï¸ Ignoring speech end in state: $currentState")
            }
        }
    }

    /**
     * å¤„ç†ç›¸æœºå¸§
     */
    private suspend fun handleCameraFrame(frame: CameraManager.CapturedFrame) {
        val frameTypeName = if (frame.type == CameraManager.FrameType.AMBIENT) "AMBIENT" else "ANCHOR"

        // ç¯å¢ƒå¸§åªåœ¨ LISTENING çŠ¶æ€æ”¶é›†
        // é”šç‚¹å¸§åœ¨ä»»ä½•çŠ¶æ€éƒ½å¤„ç†ï¼ˆå› ä¸ºå¯èƒ½åœ¨çŠ¶æ€è½¬æ¢åæ‹æ‘„ï¼‰
        if (frame.type == CameraManager.FrameType.AMBIENT &&
            _conversationState.value != ConversationState.LISTENING) {
            Log.v(TAG, "â­ï¸ Skipping AMBIENT frame in state ${_conversationState.value}")
            return
        }

        // å¤„ç†å›¾åƒ
        val result = ImageProcessor.processImage(frame.bitmap)
        if (result.isSuccess) {
            val processedImage = result.getOrNull()!!
            capturedImages.add(processedImage)

            Log.d(TAG, "ğŸ“¸ Processed $frameTypeName frame: ${processedImage.width}x${processedImage.height}, ${processedImage.sizeBytes} bytes (queue size: ${capturedImages.size})")

            // ä¿å­˜å›¾åƒåˆ°æ–‡ä»¶
            val role = when (frame.type) {
                CameraManager.FrameType.AMBIENT -> "ambient"
                CameraManager.FrameType.ANCHOR -> "anchor"
            }
            val sessionId = currentSessionId ?: return
            val imagePath = fileManager.generateImagePath(sessionId, role, imageIndex++)
            val jpegBytes = ImageProcessor.decodeDataUrl(processedImage.dataUrl) ?: return
            fileManager.saveImageFile(sessionId, imagePath, jpegBytes)

            // è®°å½•å›¾åƒåˆ°æ•°æ®åº“
            val turnId = currentUserTurnId ?: return
            val imageEntity = ImageEntity(
                turnId = turnId,
                sessionId = sessionId,
                timestamp = frame.timestamp,
                role = role,
                imagePath = imagePath,
                width = processedImage.width,
                height = processedImage.height,
                quality = processedImage.quality
            )
            database.imageDao().insertImage(imageEntity)

            Log.d(TAG, "Image saved: $imagePath")
        }
    }

    /**
     * å¼€å§‹ç”¨æˆ· turn
     */
    private suspend fun startUserTurn(timestamp: Long) {
        val sessionId = currentSessionId ?: return

        userAudioChunks.clear()
        capturedImages.clear()
        currentUserText = null  // æ¸…ç©ºæ–‡æœ¬ç¼“å­˜

        val turn = TurnEntity(
            sessionId = sessionId,
            speaker = "user",
            startTime = timestamp
        )
        currentUserTurnId = database.turnDao().insertTurn(turn)

        Log.d(TAG, "User turn started: $currentUserTurnId")
    }

    /**
     * ç»“æŸç”¨æˆ· turn
     */
    private suspend fun endCurrentUserTurn() = withContext(Dispatchers.IO) {
        val turnId = currentUserTurnId ?: return@withContext
        val sessionId = currentSessionId ?: return@withContext

        // åˆå¹¶éŸ³é¢‘æ•°æ®ï¼ˆåˆ›å»ºå‰¯æœ¬é¿å…å¹¶å‘ä¿®æ”¹å¼‚å¸¸ï¼‰
        val audioChunksCopy = userAudioChunks.toList()
        val audioData = AudioProcessor.mergeAudioChunks(audioChunksCopy)

        // ä¿å­˜éŸ³é¢‘æ–‡ä»¶
        val audioPath = if (audioData.isNotEmpty()) {
            val path = fileManager.generateAudioPath(sessionId, "user", userAudioIndex++)
            val wavData = AudioProcessor.pcm16ToWav(audioData)
            fileManager.saveAudioFile(sessionId, path, wavData)
            path
        } else {
            null
        }

        // æ›´æ–° turn è®°å½•ï¼ˆåŒ…å«éŸ³é¢‘å’Œæ–‡æœ¬ï¼‰
        val endTime = System.currentTimeMillis()
        val turn = database.turnDao().getTurnById(turnId)
        if (turn != null) {
            val duration = endTime - turn.startTime
            database.turnDao().updateTurn(
                turn.copy(
                    endTime = endTime,
                    duration = duration,
                    audioPath = audioPath,
                    text = currentUserText  // ä¿å­˜ç”¨æˆ·è¯­éŸ³è½¬å½•æ–‡æœ¬
                )
            )
            Log.d(TAG, "User turn saved: audio=${audioPath != null}, text=${currentUserText != null}")
        }

        currentUserTurnId = null
        currentUserText = null  // æ¸…ç©ºæ–‡æœ¬ç¼“å­˜
        Log.d(TAG, "User turn ended: $turnId")
    }

    /**
     * å¼€å§‹æ¨¡å‹ turn
     */
    private suspend fun startModelTurn(timestamp: Long) {
        val sessionId = currentSessionId ?: return

        modelAudioChunks.clear()
        currentModelText = null  // æ¸…ç©ºæ–‡æœ¬ç¼“å­˜

        val turn = TurnEntity(
            sessionId = sessionId,
            speaker = "model",
            startTime = timestamp
        )
        currentModelTurnId = database.turnDao().insertTurn(turn)

        Log.d(TAG, "Model turn started: $currentModelTurnId")
    }

    /**
     * ç»“æŸæ¨¡å‹ turn
     */
    private suspend fun endCurrentModelTurn(interrupted: Boolean = false) = withContext(Dispatchers.IO) {
        val turnId = currentModelTurnId ?: return@withContext
        val sessionId = currentSessionId ?: return@withContext

        // åˆå¹¶éŸ³é¢‘æ•°æ®ï¼ˆåˆ›å»ºå‰¯æœ¬é¿å…å¹¶å‘ä¿®æ”¹å¼‚å¸¸ï¼‰
        val audioChunksCopy = modelAudioChunks.toList()
        val audioData = AudioProcessor.mergeAudioChunks(audioChunksCopy)

        // ä¿å­˜éŸ³é¢‘æ–‡ä»¶
        val audioPath = if (audioData.isNotEmpty()) {
            val path = fileManager.generateAudioPath(sessionId, "model", modelAudioIndex++)
            val wavData = AudioProcessor.pcm16ToWav(audioData)
            fileManager.saveAudioFile(sessionId, path, wavData)
            path
        } else {
            null
        }

        // æ›´æ–° turn è®°å½•ï¼ˆåŒ…å«éŸ³é¢‘å’Œæ–‡æœ¬ï¼‰
        val endTime = System.currentTimeMillis()
        val turn = database.turnDao().getTurnById(turnId)
        if (turn != null) {
            val duration = endTime - turn.startTime
            database.turnDao().updateTurn(
                turn.copy(
                    endTime = endTime,
                    duration = duration,
                    audioPath = audioPath,
                    text = currentModelText,  // ä¿å­˜æ–‡æœ¬
                    interrupted = interrupted
                )
            )
            Log.d(TAG, "Model turn saved: audio=${audioPath != null}, text=${currentModelText != null}")
        }

        currentModelTurnId = null
        currentModelText = null  // æ¸…ç©ºæ–‡æœ¬ç¼“å­˜
        Log.d(TAG, "Model turn ended: $turnId (interrupted=$interrupted)")
    }

    /**
     * âœ… æ–¹æ¡ˆAï¼šå‘é€éŸ³é¢‘ + æ–‡æœ¬æŒ‡ä»¤ + å›¾ç‰‡ï¼ˆåœ¨åŒä¸€ä¸ª conversation.item.create ä¸­ï¼‰
     */
    private fun sendAudioAndImages() = scope.launch(Dispatchers.IO) {
        val sessionId = currentSessionId ?: run {
            Log.e(TAG, "âŒ Cannot send content: no active session")
            return@launch
        }

        // æ„å»ºå†…å®¹åˆ—è¡¨
        val contents = mutableListOf<ClientMessage.ConversationItemCreate.Content>()

        // 1ï¸âƒ£ æ·»åŠ éŸ³é¢‘ï¼ˆå¦‚æœæœ‰ï¼‰
        if (userAudioChunks.isNotEmpty()) {
            Log.d(TAG, "ğŸ™ï¸ Merging ${userAudioChunks.size} audio chunks...")

            // åˆå¹¶æ‰€æœ‰éŸ³é¢‘å—
            val fullAudio = AudioProcessor.mergeAudioChunks(userAudioChunks)
            val audioDurationMs = AudioProcessor.calculateDurationMs(fullAudio)

            // è½¬ä¸º Base64
            val base64Audio = AudioProcessor.pcm16ToBase64(fullAudio)

            Log.d(TAG, "  ğŸ“Š Total audio: ${fullAudio.size} bytes, ~${audioDurationMs}ms")

            // æ·»åŠ åˆ°å†…å®¹
            contents.add(
                ClientMessage.ConversationItemCreate.Content(
                    type = "input_audio",
                    audio = base64Audio
                )
            )

            // ä¿å­˜éŸ³é¢‘å‰¯æœ¬ç”¨äºè°ƒè¯•
            try {
                val audioPath = fileManager.generateAudioPath(sessionId, "user", userAudioIndex++)
                val wavData = AudioProcessor.pcm16ToWav(fullAudio)
                fileManager.saveAudioFile(sessionId, audioPath, wavData)
                Log.d(TAG, "  ğŸ’¾ Saved audio debug copy: $audioPath")
            } catch (e: Exception) {
                Log.e(TAG, "  âš ï¸ Failed to save audio: ${e.message}")
            }

            // æ¸…ç©ºéŸ³é¢‘ç¼“å†²ï¼ˆé‡è¦ï¼ï¼‰
            userAudioChunks.clear()
        } else {
            Log.w(TAG, "âš ï¸ No audio chunks to send")
        }

        // 2ï¸âƒ£ ä¸æ·»åŠ å›ºå®šè¯­è¨€çš„æ–‡æœ¬æŒ‡ä»¤
        // âœ… è®© AI æ ¹æ®ç”¨æˆ·éŸ³é¢‘çš„è¯­è¨€è‡ªåŠ¨åŒ¹é…å“åº”è¯­è¨€
        // ç³»ç»Ÿ prompt å·²ç»åŒ…å«äº† "Always respond in the SAME LANGUAGE as the user's input"
        // å¦‚æœç”¨æˆ·è¯´ä¸­æ–‡ï¼ŒAI ä¼šç”¨ä¸­æ–‡å›å¤ï¼›å¦‚æœè¯´è‹±æ–‡ï¼ŒAI ä¼šç”¨è‹±æ–‡å›å¤
        Log.d(TAG, "ğŸ“ No explicit text instruction - let AI match user's language from audio")

        // 3ï¸âƒ£ æ·»åŠ å›¾ç‰‡ï¼ˆå¦‚æœæœ‰ï¼‰
        if (capturedImages.isNotEmpty()) {
            val latestImage = capturedImages.last()

            Log.d(TAG, "ğŸ“¸ Adding latest image (from ${capturedImages.size} captured):")
            Log.d(TAG, "  ğŸ“· Image: ${latestImage.width}x${latestImage.height}, ${latestImage.sizeBytes} bytes, quality=${latestImage.quality}")

            // ä¿å­˜å›¾ç‰‡å‰¯æœ¬ç”¨äºè°ƒè¯•
            try {
                val debugPath = fileManager.generateImagePath(sessionId, "sent_to_openai", 0)
                val jpegBytes = ImageProcessor.decodeDataUrl(latestImage.dataUrl)
                if (jpegBytes != null) {
                    fileManager.saveImageFile(sessionId, debugPath, jpegBytes)
                    Log.d(TAG, "  ğŸ’¾ Saved image debug copy: $debugPath")
                }
            } catch (e: Exception) {
                Log.e(TAG, "  âš ï¸ Failed to save image: ${e.message}")
            }

            // æ·»åŠ åˆ°å†…å®¹
            contents.add(
                ClientMessage.ConversationItemCreate.Content(
                    type = "input_image",
                    imageUrl = ClientMessage.ConversationItemCreate.ImageUrl(url = latestImage.dataUrl)
                )
            )
        } else {
            Log.w(TAG, "âš ï¸ No images to send")
        }

        // 4ï¸âƒ£ æ„å»ºå¹¶å‘é€æ¶ˆæ¯
        val message = ClientMessage.ConversationItemCreate(
            item = ClientMessage.ConversationItemCreate.Item(
                role = "user",
                content = contents
            )
        )

        val success = realtimeWebSocket.send(message)
        if (success) {
            Log.i(TAG, "âœ… Successfully sent conversation item with ${contents.size} content parts:")
            contents.forEachIndexed { index, content ->
                Log.i(TAG, "   ${index + 1}. ${content.type}")
            }
        } else {
            Log.e(TAG, "âŒ Failed to send conversation item")
        }
    }

    /**
     * å‘é€æ•è·çš„å›¾åƒï¼ˆæ—§æ–¹æ³•ï¼Œä¿ç•™ä»¥é˜²éœ€è¦ï¼‰
     */
    @Deprecated("Use sendAudioAndImages() instead", ReplaceWith("sendAudioAndImages()"))
    private fun sendCapturedImages() = scope.launch(Dispatchers.IO) {
        if (capturedImages.isEmpty()) {
            Log.w(TAG, "âš ï¸ No images to send! capturedImages is empty")
            return@launch
        }

        val sessionId = currentSessionId ?: run {
            Log.e(TAG, "âŒ Cannot send images: no active session")
            return@launch
        }

        // ğŸ¯ åªä½¿ç”¨æœ€åä¸€å¼ å›¾ç‰‡ï¼ˆæœ€æ–°çš„é”šç‚¹å¸§ï¼‰
        val latestImage = capturedImages.last()

        Log.d(TAG, "ğŸ“¸ Sending latest image to OpenAI (from ${capturedImages.size} captured):")
        Log.d(TAG, "  Latest image: ${latestImage.width}x${latestImage.height}, ${latestImage.sizeBytes} bytes, quality=${latestImage.quality}")

        // ğŸ” ä¿å­˜å‘é€çš„å›¾ç‰‡å‰¯æœ¬ç”¨äºè°ƒè¯•
        try {
            val debugPath = fileManager.generateImagePath(sessionId, "sent_to_openai", 0)
            val jpegBytes = ImageProcessor.decodeDataUrl(latestImage.dataUrl)
            if (jpegBytes != null) {
                fileManager.saveImageFile(sessionId, debugPath, jpegBytes)
                Log.d(TAG, "  ğŸ’¾ Saved debug copy: $debugPath")
            }

            // éªŒè¯ data URL æ ¼å¼
            val prefix = latestImage.dataUrl.take(50)
            Log.d(TAG, "  ğŸ” Data URL prefix: $prefix...")
            if (!latestImage.dataUrl.startsWith("data:image/jpeg;base64,")) {
                Log.e(TAG, "  âŒ Invalid data URL format!")
            }
        } catch (e: Exception) {
            Log.e(TAG, "  âš ï¸ Failed to save debug copy: ${e.message}")
        }

        // ğŸ” å¦‚æœæ•è·äº†å¤šå¼ å›¾ç‰‡ï¼Œè®°å½•æ‰€æœ‰å›¾ç‰‡ä¿¡æ¯ç”¨äºè°ƒè¯•
        if (capturedImages.size > 1) {
            Log.w(TAG, "âš ï¸ Multiple images captured (${capturedImages.size}), only sending the latest:")
            capturedImages.forEachIndexed { index, image ->
                Log.d(TAG, "    Image $index: ${image.width}x${image.height}, ${image.sizeBytes} bytes")
            }
        }

        val contents = listOf(
            ClientMessage.ConversationItemCreate.Content(
                type = "input_image",  // âœ… ä½¿ç”¨æ­£ç¡®çš„ç±»å‹
                imageUrl = ClientMessage.ConversationItemCreate.ImageUrl(url = latestImage.dataUrl)
            )
        )

        val message = ClientMessage.ConversationItemCreate(
            item = ClientMessage.ConversationItemCreate.Item(
                role = "user",
                content = contents
            )
        )

        realtimeWebSocket.send(message)
        Log.i(TAG, "âœ… Successfully sent latest image to OpenAI")
        Log.i(TAG, "ğŸ“‚ Debug: Check sent_to_openai_0.jpg in session folder to verify image content")
    }

    /**
     * è®°å½•äº‹ä»¶
     */
    private suspend fun recordEvent(type: String, detail: String? = null) {
        val sessionId = currentSessionId ?: return

        val event = EventEntity(
            sessionId = sessionId,
            timestamp = System.currentTimeMillis(),
            type = type,
            detail = detail
        )
        database.eventDao().insertEvent(event)
    }

    /**
     * æ·»åŠ æ¶ˆæ¯åˆ°æ˜¾ç¤ºåˆ—è¡¨
     */
    private fun addMessage(speaker: ConversationMessage.Speaker, text: String) {
        val message = ConversationMessage(
            id = "${System.currentTimeMillis()}_${speaker.name}",
            speaker = speaker,
            text = text,
            timestamp = System.currentTimeMillis()
        )
        _messages.value = _messages.value + message
        Log.d(TAG, "Added message: ${speaker.name} - $text")
    }

    /**
     * æ›´æ–°ä¼šè¯ç»Ÿè®¡
     */
    private fun updateSessionStats() {
        val sessionId = currentSessionId ?: return
        _sessionStats.value = _sessionStats.value.copy(
            sessionId = sessionId,
            startTime = _sessionStats.value.startTime ?: System.currentTimeMillis(),
            turnCount = _sessionStats.value.turnCount + 1,
            userTurns = if (currentUserTurnId != null) _sessionStats.value.userTurns + 1 else _sessionStats.value.userTurns,
            modelTurns = if (currentModelTurnId != null) _sessionStats.value.modelTurns + 1 else _sessionStats.value.modelTurns
        )
    }

    /**
     * åˆ›å»º WebSocket å›è°ƒ
     */
    private fun createWebSocketCallback(): RealtimeWebSocket.RealtimeCallback {
        return object : RealtimeWebSocket.RealtimeCallback {
            override fun onConnected() {
                Log.d(TAG, "WebSocket connected")
            }

            override fun onDisconnected(code: Int, reason: String) {
                Log.d(TAG, "WebSocket disconnected: $code - $reason")
            }

            override fun onSessionCreated(message: ServerMessage.SessionCreated) {
                Log.d(TAG, "Session created")
            }

            override fun onSessionUpdated(message: ServerMessage.SessionUpdated) {
                Log.d(TAG, "Session updated")
            }

            override fun onConversationItemCreated(message: ServerMessage.ConversationItemCreated) {
                Log.d(TAG, "Conversation item created")
            }

            override fun onSpeechStarted(message: ServerMessage.InputAudioBufferSpeechStarted) {
                Log.d(TAG, "Server detected speech start")
            }

            override fun onSpeechStopped(message: ServerMessage.InputAudioBufferSpeechStopped) {
                Log.d(TAG, "Server detected speech stop")
            }

            override fun onAudioDelta(message: ServerMessage.ResponseAudioDelta) {
                // ç¬¬ä¸€ä¸ªéŸ³é¢‘ deltaï¼šè§¦å‘çŠ¶æ€è½¬æ¢ LISTENING â†’ MODEL_SPEAKING
                if (isFirstAudioDelta) {
                    isFirstAudioDelta = false
                    Log.d(TAG, "ğŸ”Š First audio delta received, transitioning to MODEL_SPEAKING")
                    stateMachine.handleEvent(ConversationEvent.ModelStart)

                    // å¼€å§‹æ¨¡å‹ turn
                    scope.launch {
                        startModelTurn(System.currentTimeMillis())
                    }
                }

                // è§£ç éŸ³é¢‘æ•°æ®
                val audioData = AudioProcessor.base64ToPcm16(message.delta)
                modelAudioChunks.add(audioData)

                // å†™å…¥éŸ³é¢‘è¾“å‡º
                audioOutputManager.writeAudio(audioData)
            }

            override fun onAudioDone(message: ServerMessage.ResponseAudioDone) {
                Log.d(TAG, "Audio done")
            }

            override fun onTextDelta(message: ServerMessage.ResponseTextDelta) {
                Log.d(TAG, "Text delta: ${message.delta}")
                // æ–‡æœ¬å¢é‡æš‚ä¸æ˜¾ç¤ºï¼Œç­‰å¾…å®Œæ•´æ–‡æœ¬
            }

            override fun onTextDone(message: ServerMessage.ResponseTextDone) {
                Log.d(TAG, "Text done: ${message.text}")
                // ç¼“å­˜æ–‡æœ¬ï¼Œç¨åä¿å­˜åˆ°æ•°æ®åº“
                currentModelText = message.text
                // æ·»åŠ æ¨¡å‹å›å¤åˆ°æ¶ˆæ¯åˆ—è¡¨
                if (message.text.isNotEmpty()) {
                    addMessage(ConversationMessage.Speaker.MODEL, message.text)
                }
            }

            override fun onResponseDone(message: ServerMessage.ResponseDone) {
                scope.launch {
                    val status = message.response.status
                    Log.d(TAG, "Response done with status: $status")

                    when (status) {
                        "completed" -> {
                            // æ­£å¸¸å®Œæˆ
                            stateMachine.handleEvent(ConversationEvent.ModelEnd)
                            endCurrentModelTurn()
                            recordEvent("MODEL_END", "Model finished speaking (status: $status)")
                        }
                        "cancelled" -> {
                            // å“åº”è¢«å–æ¶ˆï¼ˆé€šå¸¸æ˜¯è¢«æ‰“æ–­ï¼‰
                            Log.w(TAG, "Response was cancelled")
                            stateMachine.handleEvent(ConversationEvent.ModelEnd)
                            endCurrentModelTurn(interrupted = true)
                            recordEvent("MODEL_CANCELLED", "Model response cancelled")
                        }
                        "failed", "incomplete" -> {
                            // å“åº”å¤±è´¥æˆ–ä¸å®Œæ•´
                            Log.e(TAG, "Response failed with status: $status, details: ${message.response.statusDetails}")
                            stateMachine.handleEvent(ConversationEvent.ModelEnd)
                            endCurrentModelTurn()
                            recordEvent("MODEL_ERROR", "Model response failed: $status")

                            // å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ é”™è¯¯é€šçŸ¥ç»™ç”¨æˆ·
                            addMessage(
                                ConversationMessage.Speaker.MODEL,
                                "[ç³»ç»Ÿ] å“åº”å¼‚å¸¸ï¼ŒçŠ¶æ€: $status"
                            )
                        }
                        else -> {
                            // æœªçŸ¥çŠ¶æ€
                            Log.w(TAG, "Unknown response status: $status")
                            stateMachine.handleEvent(ConversationEvent.ModelEnd)
                            endCurrentModelTurn()
                            recordEvent("MODEL_UNKNOWN_STATUS", "Unknown status: $status")
                        }
                    }
                }
            }

            override fun onInputAudioTranscriptionCompleted(message: ServerMessage.InputAudioTranscriptionCompleted) {
                Log.d(TAG, "User transcription completed: ${message.transcript}")
                // ç¼“å­˜ç”¨æˆ·è¯­éŸ³è½¬å½•æ–‡æœ¬
                currentUserText = message.transcript
                // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°æ¶ˆæ¯åˆ—è¡¨
                if (message.transcript.isNotEmpty()) {
                    addMessage(ConversationMessage.Speaker.USER, message.transcript)
                }
            }

            override fun onServerError(message: ServerMessage.Error) {
                Log.e(TAG, "Server error: ${message.error.message}")
            }

            override fun onError(error: Throwable) {
                Log.e(TAG, "Client error", error)
            }

            override fun onMaxReconnectAttemptsReached() {
                Log.e(TAG, "Max reconnect attempts reached")
            }
        }
    }

    /**
     * é‡Šæ”¾èµ„æº
     */
    fun release() {
        scope.cancel()
        audioInputManager.release()
        audioOutputManager.release()
        vadDetector.release()
        cameraManager.release()
        realtimeWebSocket.release()
        Log.d(TAG, "ConversationManager released")
    }
}
